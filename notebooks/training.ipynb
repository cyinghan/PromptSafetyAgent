{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.agent import *\n",
    "\n",
    "# Download and save the safe-guard-prompt-injection dataset\n",
    "raw_dataset = load_dataset(\"xTRam1/safe-guard-prompt-injection\", cache_dir=DATA_DIR)\n",
    "cleaned_dataset = raw_dataset.map(clean_text)\n",
    "\n",
    "training_prompts = [row['text'] for row in cleaned_dataset['train']]\n",
    "training_prompt_lens = [count_words(row['text']) for row in raw_dataset['train']]\n",
    "training_prompt_labels = [row['label'] for row in cleaned_dataset['train']]\n",
    "\n",
    "test_prompts = [row['text'] for row in cleaned_dataset['test']]\n",
    "test_prompt_lens = [count_words(row['text']) for row in raw_dataset['test']]\n",
    "test_prompt_labels = [row['label'] for row in cleaned_dataset['test']]\n",
    "\n",
    "print(f\"Test/Train size: {len(cleaned_dataset['test'])}/{len(cleaned_dataset['train'])}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_duplicates(training_prompts)",
   "id": "b29d7e34c9672b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_unique_texts = set(training_prompts)\n",
    "test_unique_texts = set(test_prompts)\n",
    "\n",
    "# Find overlaps\n",
    "overlap = train_unique_texts.intersection(test_unique_texts)\n",
    "print(f\"Number of overlapping samples: {len(overlap)}\")\n",
    "\n",
    "# Print all prompts which exists in both training and test set\n",
    "# Observations:\n",
    "for text in overlap:\n",
    "    for row in raw_dataset['train']:\n",
    "        if row['text'] == text:\n",
    "            print(f\"From Train - Label: {row['label']} for Overlap Text: {text[:100]}\")\n",
    "\n",
    "    for row in raw_dataset['test']:\n",
    "        if row['text'] == text:\n",
    "            print(f\"From Test  - Label: {row['label']} for Overlap Text: {text[:100]}\")"
   ],
   "id": "d8df5cf4c3f84de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_label_percentages = pd.Series(test_prompt_labels).value_counts(normalize=True) * 100\n",
    "training_label_percentages = pd.Series(training_prompt_labels).value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(training_prompt_lens, bins=100)\n",
    "plt.xlim(0,2000)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Training Prompt Length Histogram')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.pie(training_label_percentages.values, labels=training_label_percentages.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Classification Training Labels')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(test_prompt_lens, bins=100)\n",
    "plt.xlim(0,2000)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Test Prompt Length Histogram')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.pie(test_label_percentages.values, labels=test_label_percentages.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Classification Test Labels')\n",
    "plt.show()"
   ],
   "id": "5416f2924f995fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tfidf_dataset = cleaned_dataset.map(lambda x: {\"text\": preprocess_spacy(x[\"text\"])})\n",
    "vectorizer = TfidfVectorizer(max_features=25000)\n",
    "X_train = vectorizer.fit_transform(tfidf_dataset[\"train\"][\"text\"])\n",
    "X_test = vectorizer.transform(tfidf_dataset[\"test\"][\"text\"])\n",
    "y_train = cleaned_dataset[\"train\"][\"label\"]\n",
    "y_test = cleaned_dataset[\"test\"][\"label\"]\n",
    "save_with_pickle(vectorizer, f\"{MODEL_DIR}/vectorizer.pkl\")\n",
    "\n",
    "print(\"TF-IDF shape:\", X_train.shape,)  # (num_samples, num_features)\n",
    "\n",
    "# Train Logistic Regression\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "misclassified = []\n",
    "for i, (gt, pred) in enumerate(zip(y_test, y_pred)):\n",
    "    if gt != pred:\n",
    "        misclassified.append({\n",
    "            \"prompt\": cleaned_dataset[\"test\"][\"text\"][i],\n",
    "            \"ground_truth\": gt,\n",
    "            \"prediction\": pred\n",
    "        })\n",
    "\n",
    "# Display Mismatched Labels for baseline logistic model\n",
    "print(\"Total Misclassified:\", len(misclassified))\n",
    "for i, item in enumerate(misclassified):\n",
    "    print(f\"\\n{separator(f\"Prompt {i+1} Summary\")}\\nGround Truth Label: {item['ground_truth']} - Predicted Label: {item['prediction']} - Prompt Word Length: {count_words(item['prompt'])}\", f\"\\n{separator(\"Prompt\")}\\n {item['prompt']}\")\n",
    "print(\"\\n\", separator())"
   ],
   "id": "812952b05e81b3ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "misclassified_prompt_len = [count_words(item['prompt']) for item in misclassified]\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.hist(misclassified_prompt_len, bins=100)\n",
    "plt.xlim(0,2000)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Misclassified Prompt Length Histogram')\n",
    "plt.show()"
   ],
   "id": "72f1f0b99847c960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.evaluate import *\n",
    "\n",
    "raw_train_valid = cleaned_dataset[\"train\"].train_test_split(test_size=VALIDATION_SPLIT, seed=SEED)\n",
    "dataset_split = {\n",
    "    \"train\": raw_train_valid[\"train\"],\n",
    "    \"validation\": raw_train_valid[\"test\"],\n",
    "    \"test\": cleaned_dataset[\"test\"]\n",
    "}\n",
    "# Load tokenizer and split the dataset into training and validation\n",
    "bert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', cache_dir=MODEL_DIR)\n",
    "bert_tokenized_dataset = {\n",
    "    split: dataset_split[split].map(lambda x: bert_tokenizer(x['text'], truncation=True, max_length=512),\n",
    "                                    batched=True) for split in dataset_split}\n",
    "\n",
    "bert_train_dataset = bert_tokenized_dataset[\"train\"]\n",
    "bert_val_dataset = bert_tokenized_dataset[\"validation\"]\n",
    "bert_test_dataset = bert_tokenized_dataset[\"test\"]"
   ],
   "id": "ab2c407adc219559",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Classification Stage\n",
    "bert_model_path = f\"{MODEL_DIR}/distilbert-finetuned-psa-5eps\"\n",
    "# train_distilbert_model(bert_train_dataset, bert_val_dataset, bert_tokenizer, compute_metrics, model_path=bert_model_path, model_dir=MODEL_DIR, epoches=5)\n",
    "bert_model = fetch_distilbert_model(bert_model_path)\n",
    "\n",
    "# Run DistilBERT classifier prediction\n",
    "sliding_y_preds, sliding_y_probs = distilbert_predict(bert_test_dataset, bert_tokenizer, bert_model)\n",
    "\n",
    "# Show first 10 examples\n",
    "for i in range(10):\n",
    "    print(f\"Text: {cleaned_dataset['test'][i]['text'][:60]}...\")\n",
    "    print(f\"True Label: {cleaned_dataset['test'][i]['label']}, Predicted: {sliding_y_preds[i]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(classification_report(cleaned_dataset[\"test\"][\"label\"], sliding_y_preds, target_names=[\"negative\", \"positive\"]))"
   ],
   "id": "2deeca7caab273d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print_misclassified(y_test, sliding_y_preds, cleaned_dataset)",
   "id": "281fbb5667bfe91b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ith_row = 0\n",
    "phi_model, phi_tokenizer = setup_phi2(MODEL_DIR)"
   ],
   "id": "d823266146899e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "start_row = ith_row\n",
    "results = {\"ground_truth\":[], \"label\":[], \"score\":[], \"confidence\":[], \"fallback_used\":[], \"explanation\":[], \"recommendation\":[], \"prompt\":[]}\n",
    "total_time = 0\n",
    "exec_count = 0\n",
    "for i in range(start_row, len(cleaned_dataset['test'])):\n",
    "    print(separator(f\"Test Prompt {i}\", width=150))\n",
    "    data_row = cleaned_dataset['test'][i]\n",
    "\n",
    "    # Accumulate execution time\n",
    "    start_time = time.time()\n",
    "    json_output = run_agents(data_row['text'], decision_label=sliding_y_preds[i], reasoning_agent_info=(phi_model, phi_tokenizer), vectorizer=vectorizer, debug_mode=True)\n",
    "    total_time += time.time() - start_time\n",
    "    exec_count += 1\n",
    "    # Record fields\n",
    "    results['ground_truth'].append(cleaned_dataset['test'][\"label\"][i])\n",
    "    results['label'].append(sliding_y_preds[i])\n",
    "    results['score'].append(sliding_y_probs[i])\n",
    "    results['confidence'].append(json_output['confidence_score'])\n",
    "    results['fallback_used'].append(json_output['confidence_score'] <= 0.5)\n",
    "    results['explanation'].append(json_output['reasoning'])\n",
    "    results['recommendation'].append(json_output['recommendation'])\n",
    "    results['prompt'].append(data_row['text'])\n",
    "    ith_row += 1\n",
    "print(f\"average elapsed time: {total_time/exec_count}\")"
   ],
   "id": "8c5e81a9b3b40e58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save Result to data\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(os.path.join(DATA_DIR, \"results.csv\"))"
   ],
   "id": "201b35f4d245ad31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result_df[result_df['confidence'] <= 0.3][['ground_truth', 'label', \"prompt\", 'explanation']]\n",
    "# filtered_df[filtered_df['label'] != filtered_df['ground_truth']]"
   ],
   "id": "67cf4663a749edf0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
